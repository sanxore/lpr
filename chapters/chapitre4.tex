\chapter{Modélisation des données et Évaluation}
\epigraph{Economists and agronomists are locked in debate about likely
future yields. Since the method of the economists is to predict
future outcomes from past performance, economists expect
success to continue. And since for the scientists future success
depends on discoveries they will have to make and do not now
know how to make, the scientists are doubtful. At its core, this is
a disagreement about the pace of technical change.}{Robert
Socolow}
\cleardoublepage

	\section{Analyse causale}
		\shorthandoff{!}
		\lstset{language=R}
		\shorthandon{!}
	\subsection{Introduction à la régression en composantes principales}
	Les méthodes de régression multivariée comme la RCP\footnote{Régression en composantes principales} jouissent d'une large popularité dans divers domaines d'études, y compris les sciences naturelles. La raison principale de ce succès étant qu'elle ont été conçues dans l'optique de faire face à des situations où on dispose de plusieurs variables exogènes, généralement hautement corrélées, et relativement peu d'échantillons. Une situation qui est justement notre problématique. Ainsi une forme de réduction de dimensions dans l'espace des exogènes peut grandement simplifier le problème de régression.L'analyse en composantes principales est une technique de réduction de dimensions où une combinaison linéaire de \textit{p} variables indépendantes/orthogonales des paramètres intrants $X_1,X_2,...X_p$ sont créées de façon à ce que la première combinaison linéaire $Z_{1}$ (composante principale) capture autant de la variance totale des données d'origine. La $2^{de}$ $Z_{2}$ capture autant que possible du reste de la variance sous la réserve que celle-ci soit orthogonale(i.e. non corrélée) à la première. En général toutes les variables intrantes sont standardisées de façon à posséder une moyenne nulle et une variance unitaire que l'on notera $X_{j}^{*}$. Ainsi la variance totale des \textit{p} intrants standardisés est donnée par :
	\begin{equation*}
	\sum_{j=1}^{p} V(X_{j}^{*}) = p = \sum_{j=1}^{p}V(Z_{j})
	\end{equation*}
	avec $\forall i \in [1,p]$:
	\begin{equation*}
	Z_i = \sum_{j=1}^{p} a_{ij} X_{j}^{*} 
	\end{equation*}
	et $\forall i \neq j$
	\begin{equation*}
		Cov(Z_i,Z_j) = Corr(Z_i,Z_j) = 0
	\end{equation*}
	Les composantes principales sont déterminées par l'analyse spectrale (i.e la recherche des valeurs propres et vecteurs propres) de la matrice des corrélations des variables intrantes. La variance de la $j^{eme}$ composante principale $Z_{j}$ est $\lambda_{j}$ la $j^{eme}$ valeur propre la plus grande. Les coefficients $a_{ij}$ de la combinaison linéaire sont les vecteurs propres correspondant.\par
	Idéalement les \textit{k} premières composantes principales comprendront une part consistante de la variance totale des données. Nous pouvons par la suite utiliser ces \textit{k} principales composantes $Z_1,Z_2,...X_k$ comme variables exogènes dans le modèle de régression linéaire multiple:
		\begin{equation*}
		Y=\beta_0 + \sum_{\j=1}^{k} \beta_jZ_j + \epsilon 
		\end{equation*}
	Dans ce qui suit nous mettons en œuvre une RCP sur nos données élaguées obtenus après le traitement du chapitre précédent. Le script en langage \textbf{R}, avec lequel la RCP est automatisée, est disponible dans le dossier \textit{Source} de notre dépôt PFE \textbf{GitHub}\cite{this}.
	\subsection{Mise en œuvre de la régression en composantes principales}
	\subsubsection{Corrélations croisées des variables exogènes}
	Nous commençons d'abord par charger les données quantitatives des pays. Il s'agit de l'élagage des données externes présentées à la section \ref{exoList}. Ces données représentent, de la totalité des 39 variables initiales, les 12 variables retenues en sortie de la procédure introduite par les forets aléatoires en section \ref{faout} avant de créer deux matrices:\begin{itemize}
	\item DF.actifs = Cette matrice accueillera les 12 variables retenues dans la figure \ref{fig:laggedvars}.
	\item DF.illus = Cette matrice-vecteur accueillera notre variable de sortie qui est la \textbf{Consommation de kilogrammes de fertilisants phosphatés par hectare de terre arable} 
	\end{itemize}
	Commençons d'abord à chercher les variables relativement colinéaires. Nous donnons dans la figure \ref{fig:scatter} les diagrammes de dispersions deux-à-deux croisés entre les 12 variables de notre modélisation. Nous joignons à ceux-ci des courbes splines facilitant la détection de structure ainsi qu'en gras les valeurs des coefficients de corrélations les plus importants.
			\begin{figure}[h]
				    		\centering
				    		\fbox{\includegraphics[width=\linewidth]{ch4-images/scatter}}
				    		\caption{Diagrammes des dispersions croisées, courbes splines et corrélations des variables exogènes}
				    		\label{fig:scatter}
			\end{figure}
	Plusieurs couples de valeurs présentent des corrélations sensiblement élevées en valeurs absolues:
	\begin{itemize}
	\item \{Coût d'import conteneur Vs Coût d'export conteneur\}
	\item \{\%(Agriculture) du PIB Vs Espérance de vie\}
	\item \{PIB par habitant Vs Indice de performance logistique\}
	\item \{\%(Agriculture) du PIB Vs \%(Population rurale) de la population totale\}
	\item \{Indice de performance logistique Vs Espérance de vie\}
	\item Certaines de ces variables entre elles par transition des couple ci-dessus.
	\end{itemize}
	Une autre façon d'examiner la structure des corrélations est la création de carte coloriée dressant les corrélations deux à deux entre les variables numériques. Ceci sont \textit{clusturisés} et ordonnés aux seins de groupes de variables corrélées. Sur la figure \ref{fig:heatmap} nous pouvons distinguer 4 à 5 groupes de variables exogènes corrélées. Ainsi une ACP devrait produire 4 à 5 composante principales intégrant la majorité de l'information contenue dans les données.
				\begin{figure}[h]
					    		\centering
					    		\fbox{\includegraphics[width=\linewidth]{ch4-images/heatmap}}
					    		\caption{Carte coloriée des regroupement des variables corrélées}
					    		\label{fig:heatmap}
				\end{figure}
	\subsubsection{Calcul des composantes principales des données élaguées}
	Nous commençons par effectuer une décomposition spectrale de la matrice de corrélation. La figure \ref{fig:valpropres} ci-dessous liste les valeurs propres résultant de cette décomposition.
				\begin{figure}[h]
					    		\centering
					    		\fbox{\includegraphics[]{ch4-images/valeurs_propres}}
					    		\caption{Valeurs propres de la décomposition spectrale de la matrice de corrélation}
					    		\label{valpropres}
				\end{figure}
	La figure \ref{fig:inert} suivante montre que 70\% et 78\% de l'inertie de l'ensemble des données est exp
	\section{Analyse en séries chronologiques et projections}
	Étude de la série "Demande en fertilisant du brésil "
	\begin{figure}[H]
		\centering
		\fbox{\includegraphics[width=\textwidth]{ch4-images/serie}}
		\caption{Demande en kilo-tonne de fertilisant "Brésil"}
		\label{fig:dem}
	\end{figure}
	\subsection{Décomposition de la série}
	\begin{figure}[H]
		\centering
		\fbox{\includegraphics[width=\textwidth]{ch4-images/decomp}}
		\caption{Le tracé de la décomposition "Tendance + saisonnalité + résidu"}
		\label{fig:dec}
	\end{figure}
	La figure ci-dessus montre la série originale (en haut), la tendance estimée (deuxième), la composante saisonnière estimée (troisième) et la composante irrégulière estimée (en bas).On remarque que la tendance est croissante.
	\subsection{Lissage et prévision par la méthode de Holt-Winters}
	\begin{figure}[H]
		\centering
		\fbox{\includegraphics[width=0.5\textwidth]{ch4-images/hw_param}}
		\caption{Paramètre de lissage et coefficients de saisonnalité}
		\label{fig:hw_param}
	\end{figure}
	The estimated values of alpha, beta and gamma are 0.41, 0.00, and 0.96, respectively. The value of alpha (0.41)
is relatively low, indicating that the estimate of the level at the current time point is based upon both recent
observations and some observations in the more distant past. The value of beta is 0.00, indicating that the estimate
of the slope b of the trend component is not updated over the time series, and instead is set equal to its initial value.
This makes good intuitive sense, as the level changes quite a bit over the time series, but the slope b of the trend
component remains roughly the same. In contrast, the value of gamma (0.96) is high, indicating that the estimate
of the seasonal component at the current time point is just based upon very recent observations.
\par
As for simple exponential smoothing and Holt’s exponential smoothing, we can plot the original time series as a
black line, with the forecasted values as a red line on top of that:
	\begin{figure}[H]
		\centering
		\fbox{\includegraphics[width=\textwidth]{ch4-images/hw_for}}
		\caption{Courbe estimé par la méthode Holt-Winters.}
		\label{fig:hw_for}
	\end{figure}
	We see from the plot that the Holt-Winters exponential method is very successful in predicting the seasonal peaks.
	
	\begin{figure}[H]
		\centering
		\fbox{\includegraphics[width=\textwidth]{ch4-images/forcast}}
		\caption{Prédiction pour les quatre ans à venir:}
		\label{fig:forecast}
	\end{figure}
	The forecasts are shown as a blue line, and the orange and yellow shaded areas show 80\% and 95\% prediction
intervals, respectively.

\par
We can investigate whether the predictive model can be improved upon by checking whether the in-sample forecast
errors show non-zero autocorrelations at lags 1-20, by making a correlogram and carrying out the Ljung-Box test:
	\begin{figure}[H]
		\centering
		\fbox{\includegraphics[width=.5\textwidth]{ch4-images/Ljung-Box}}
		\caption{Test de Ljung-Box}
		\label{fig:Q-test}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\fbox{\includegraphics[width=\textwidth]{ch4-images/correlogram}}
		\caption{correlogram}
		\label{fig:correlogram}
	\end{figure}
	
	The correlogram shows that the autocorrelations for the in-sample forecast errors do not exceed the significance
bounds for lags 1-20. Furthermore, the p-value for Ljung-Box test is 0.6, indicating that there is little evidence of
non-zero autocorrelations at lags 1-20.


\par
We can check whether the forecast errors have constant variance over time, and are normally distributed with
mean zero, by making a time plot of the forecast errors and a histogram (with overlaid normal curve):


\begin{figure}[H]
		\centering
		\fbox{\includegraphics[width=\textwidth]{ch4-images/plot-resi}}
		\caption{Le tracé des résidu}
		\label{fig:plot-resi}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\fbox{\includegraphics[width=\textwidth]{ch4-images/hist-resi}}
		\caption{Histogramme des résidu}
		\label{fig:hist-resi}
	\end{figure}
	
	
	From the time plot, it appears plausible that the forecast errors have constant variance over time. From the
histogram of forecast errors, it seems plausible that the forecast errors are normally distributed with mean zero.
\par
Thus,there is little evidence of autocorrelation at lags 1-20 for the forecast errors, and the forecast errors appear
to be normally distributed with mean zero and constant variance over time. This suggests that Holt-Winters
exponential smoothing provides an adequate predictive model of the log of sales at the souvenir shop, which
probably cannot be improved upon. Furthermore, the assumptions upon which the prediction intervals were based
are probably valid.
