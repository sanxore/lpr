	\chapter*{Annexe A1 : Introduction aux forêts de décision aléatoires}
	Considérons un ensemble d'apprentissage ${L_n = \{(X_1, Y_1),...,X_n, Y_n)\}}$ de \textit{n} observations i.i.d.\footnote{indépendantes identiquement distribuées} d'un vecteur aléatoire (X,Y). Le vecteur ${X_i = (X_i^1,...,X_i^p)}$ contient les variables exogènes, $X_i \in \reels^p $ et $Y_i \in \reels $, une réponse numérique. Pour les problèmes de regression, nous supposons que $ \exists$ S $\forall$ i $ Y_i=S(X_i)+\varepsilon$ où E[$\varepsilon$|X] = 0 et \textbf{S} est appellée fonction de regression. Les FA est une stratégie de construction d'un modèle \textit{ê(x)} estimant la fonction de regression.
	\par
	Les méthodes d’agrégation consistent à agréger un nombre \textit{B} d’estimateurs $ê_1,...,ê_\textit{B}$ : ${ê(x) = ê_B(x) =  \frac{1}{B} \sum_{i=1}^{B} ê_i}$. On considère l’erreur quadratique moyenne d’un estimateur \textit{ê} et sa décomposition biais-variance :
	\begin{center}
	${E[(ê(x)-S(x))^2] = (E[ê(x)] - S(x))^2 + Var(ê(x))}$
	\end{center}
	Si on suppose les régrésseurs $ê_1,...,ê_B$ i.i.d on a :
	\begin{center}
		$E[ê(x)] = E[ê_1(x)]$ et $Var(ê(x)) = \frac{1}{B} Var(ê_1(x)$
	\end{center}
	Le biais de l’estimateur agrégé est donc le même que celui des $ê_k(x)$ mais la variance diminue. Bien
	entendu, en pratique il est quasiment impossible de considérer des estimateurs $ê_k(x)$ indépendants
	dans la mesure où ils dépendent tous du même échantillon \textit{$L_n$} . L’approche des FA consiste à
	tenter d’atténuer la dépendance entre les estimateurs que l’on agrège en les construisant sur des
	échantillons bootstrap\footnote{Ré-échantillonnage}. Nous référerons le lecteur à l'annexe A4, pour une démonstration.
